# Script principal de treino do bot
import os
import pandas as pd
import gym
from stable_baselines3 import PPO
from env.trading_env import TradingEnv
from data.downloader import download_data
from indicators.base_indicators import (
    calculate_rsi, calculate_macd, calculate_sma, calculate_ema,
    calculate_stochastic_oscillator, calculate_adx
)
from utils.config import load_config

def prepare_data(ticker, start_date="2010-01-01", end_date="2025-01-01"):
    """
    Download and prepare stock data for training.
    :param ticker: Stock symbol (e.g., 'AAPL')
    :param start_date: Start date for historical data
    :param end_date: End date for historical data
    :return: DataFrame with prepared data
    """
    # Download the stock data
    df = download_data(ticker, start_date, end_date)

    # Calculate technical indicators
    df = calculate_rsi(df)
    df = calculate_macd(df)
    df = calculate_sma(df)
    df = calculate_ema(df)
    df = calculate_stochastic_oscillator(df)
    df = calculate_adx(df)

    # Drop NaN values generated by indicators
    df = df.dropna()

    return df

def train_model(ticker, start_date="2010-01-01", end_date="2025-01-01"):
    """
    Train the reinforcement learning model (PPO) on stock data.
    :param ticker: Stock symbol (e.g., 'AAPL')
    :param start_date: Start date for historical data
    :param end_date: End date for historical data
    :return: Trained PPO model
    """
    # Prepare the data
    df = prepare_data(ticker, start_date, end_date)

    # Create the trading environment
    env = TradingEnv(df)

    # Initialize the PPO model with the environment
    model = PPO("MlpPolicy", env, verbose=1)

    # Train the model
    model.learn(total_timesteps=20000)

    # Save the trained model
    model.save(f"models/{ticker}_ppo_trained")

    print(f"Model trained and saved as {ticker}_ppo_trained")

    return model

def evaluate_model(ticker):
    """
    Evaluate the trained PPO model on a test set.
    :param ticker: Stock symbol (e.g., 'AAPL')
    :return: Evaluation results
    """
    # Load the trained model
    model = PPO.load(f"models/{ticker}_ppo_trained")

    # Download and prepare the test data
    df_test = prepare_data(ticker, start_date="2022-01-01", end_date="2025-01-01")

    # Create the trading environment for testing
    env_test = TradingEnv(df_test)

    # Evaluate the model
    obs = env_test.reset()
    done = False
    total_rewards = 0

    while not done:
        action, _ = model.predict(obs)
        obs, reward, done, info = env_test.step(action)
        total_rewards += reward

    print(f"Total rewards from test: {total_rewards}")

def main():
    config = load_config()

    ticker = config['stock_ticker']  # Ticker of the stock to train on
    start_date = config['start_date']
    end_date = config['end_date']

    # Train the model
    model = train_model(ticker, start_date, end_date)

    # Evaluate the model
    evaluate_model(ticker)

if __name__ == "__main__":
    main()
